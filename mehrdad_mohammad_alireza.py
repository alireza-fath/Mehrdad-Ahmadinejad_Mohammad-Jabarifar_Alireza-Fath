# -*- coding: utf-8 -*-
"""Mehrdad_Mohammad_Alireza.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-3d1lxKnsxRnpoKp4QFO3NExQIGaPVbD
"""

# Connecting to google colab
from google.colab import drive
drive.mount('/content/drive/')

## Mounting the Drive

import os

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'CS254/CS254-Project/' # change this directory to yours
GOOGLE_DRIVE_PATH = os.path.join('drive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print(os.listdir(GOOGLE_DRIVE_PATH))

# importing libraries

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn import metrics
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import r2_score
from sklearn import linear_model
from sklearn.linear_model import LinearRegression

# Load the data and store it in a pandas dataframe

# path = GOOGLE_DRIVE_PATH + '/MLProjectcsvout1.csv'
path = GOOGLE_DRIVE_PATH + '/MLProjectcsvout2.csv'
# data = pd.read_csv(path, header=None, names=['FlowRate', 'VolumeFraction', 'SolarRadiation', 'ElectricalEfficiency'])
data = pd.read_csv(path, header=None, names=['FlowRate', 'VolumeFraction', 'SolarRadiation', 'ThermalEfficiency'])
data.head(n=4)

# Evaluate the Data
data.describe()

# set X (training data) and y (target variable)

cols = data.shape[1]
X = data.iloc[:,0:cols-1] # iloc slicing function 
y = data.iloc[:,cols-1:cols]
y=y.values.ravel()

X.shape, y.shape

#Center to the mean and component wise scale to unit variance.

standardized_X = preprocessing.scale(X)

# Split the data into training and testing, 80% for training and 20% for testing

X_train, X_test, y_train, y_test = train_test_split(standardized_X, y, test_size=0.20, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

# Using cross-validation to randomly find the best Hyperparameters

model=SVR(kernel="rbf", gamma="auto").fit(X_train, y_train)
y_pred_before=model.predict(X_test)

# Range of the Hyperparameters

C_range = np.logspace(0, 4, 20000)
epsilon_range = np.logspace(-8, 3, 20000)

# print (C_range)
# print (epsilon_range)

# specify parameters and distributions to sample from
param_dist = {"epsilon": epsilon_range,
              "C": C_range}

# run randomized search
n_iter_search = 5000
random_search = RandomizedSearchCV(model, param_distributions=param_dist,
                                   n_iter=n_iter_search, cv=10)

random_search.fit(X_train, y_train)

# summarize the results of the random search
print(random_search.best_score_)
print(random_search.best_estimator_.epsilon)
print(random_search.best_estimator_.C)
y_pred_after=SVR(kernel="rbf", C=random_search.best_estimator_.C, gamma="auto", epsilon=random_search.best_estimator_.epsilon).fit(X_train, y_train).predict(X_test)

# comparing the model before and after finding the best hyperparameters

score_before=r2_score(y_test, y_pred_before)
score_after=r2_score(y_test, y_pred_after)

print(score_before)
print(score_after)

# Comparing the predicted output versus the test output

# Mean_Value
mean_value=np.mean(y_train)
mean_array=np.ones(6)*mean_value
mean_score=r2_score(y_test, mean_array)

d = {'Predicted Output': y_pred_after, 'Mean Output': mean_value, 'Test Output': y_test }
df = pd.DataFrame(data=d)
df.loc[len(df.index)] = [score_after, mean_score, 'R2 Score']
df.head(7)